# 강화 학습 기반 에지 컴퓨팅 작업 오프로딩 최적화 (PPO vs 휴리스틱)

## 개요 및 실험 목표
이전 연구인([PPO_Heuristic](https://github.com/ParkMinhyeok/AD_RL_Offloading/tree/main/compare_PPO_Heuristic))의 PPO에서 발전된 에이전트를 계발하는 것입니다.
본 연구의 핵심 목표는 앞선 연구와 동일하게 **에지 컴퓨팅 환경의 부하를 동적으로 고려하여, 클라이언트와 서버 간의 작업 분배를 최적화하는 지능형 오프로딩 정책을 개발**하는 것입니다.

사용자의 요청이 계속해서 발생하는 동적인 환경에서 한정된 자원을 가진 에지 서버의 과부하를 방지하고 전체 시스템의 처리량, 학습 속도를 극대화하기 위해, **상태 번수를 추가한 강화학습(PPO) 에이전트**를 학습시키고, 이 에이전트의 성능을 규칙 기반 **휴리스틱 모델**과 비교하여, 강화학습 기반 정책의 효율성과 우수성을 검증하고자 합니다.

---

## 실험 환경

-   **시뮬레이션 기반 환경**: 데이터 전송 시간, 클라이언트 처리 시간, 서버 처리 시간 등을 모델링한 환경을 구축하여 실험을 진행합니다.
-   **상태(State)**:
    -   **기존 연구**: 서버 대기 시간, 큐 길이 (`STATE_SIZE = 2`)
    -   **본 프로젝트**: 서버 대기 시간, 큐 길이, **클라이언트 처리 시간 벡터 (다운샘플링)** (`STATE_SIZE = 13`)
-   **행동(Action)**: 작업을 클라이언트에서 처리할지, 서버의 특정 분할 지점으로 오프로딩할지를 결정합니다 (총 11가지 행동).
-   **보상(Reward)**: 작업 완료 시 **턴어라운드 타임(Turnaround Time)에 반비례**하는 보상을 제공하여 에이전트가 작업 처리 시간을 최소화하도록 유도합니다.

---

## 알고리즘

-   **PPO (Proximal Policy Optimization)**: 위에서 설계된 강화학습 환경과 상호작용하며 장기적인 보상 합을 극대화하는 정책을 학습합니다.
-   **휴리스틱**: "서버 대기 시간"이라는 단기적 정보만을 사용하여 탐욕적(Greedy)으로 분할 지점을 결정하는 규칙 기반 모델로 간단한 규칙 기반의 정책으로, 서버의 유휴 상태를 확인하여 작업을 오프로딩하는 방식을 사용합니다.

---

## 실험 결과

### 1. 최종 성능 비교

| 지표                 | PPO 에이전트 (본 프로젝트) | 휴리스틱 모델 |
| :------------------- | :-----------: | :-----------: |
| 완료된 총 작업 수   | **155 개** | 129 개        |
| 평균 응답 시간       | **129.91 초** | 131.69 초     |

*결과 그래프는 로컬에서 실행 시 생성됩니다.*
<img width="1400" height="600" alt="Figure_2" src="https://github.com/user-attachments/assets/92e0b45e-729b-453d-b8f3-5525627a5c6a" />


**PPO 에이전트**는 휴리스틱 모델에 비해 **더 많은 작업을 처리**했을 뿐만 아니라, **평균 응답 시간 또한 단축**시키는 우수한 성능을 보였습니다. 이는 PPO가 더 복잡하고 효율적인 오프로딩 전략을 학습했음을 시사합니다.

### 2. PPO 학습 과정

*학습 과정 그래프는 로컬에서 실행 시 생성됩니다.*
<img width="2000" height="500" alt="Figure_1" src="https://github.com/user-attachments/assets/ca6ca506-edb0-41be-84ff-09034065fb25" />


-   **에피소드별 누적 보상**: 학습이 진행됨에 따라 보상이 점진적으로 증가하며 수렴하는 경향을 보입니다.
-   **Actor/Critic 손실 함수**: 손실 값들이 학습이 진행됨에 따라 점차 감소하며 안정화되는 것을 확인할 수 있습니다.

---

## 주요 변경 사항

### 기존 연구의 한계

이전 연구의 에이전트는 **`[서버 대기 시간, 큐 길이]`** 라는 매우 제한적인 상태 정보만을 사용했습니다. 이로 인해 에이전트는 각 행동(분할 지점)을 선택했을 때 발생하는 클라이언트 측의 시간 비용을 알지 못했고, 이 둘 사이의 관계를 학습하는 데 매우 오랜 시간이 걸리거나 최적의 정책을 찾는 데 실패했습니다.

### 본 프로젝트의 주요 개선 사항

이러한 한계를 극복하기 위해 본 프로젝트에서는 다음과 같은 핵심적인 개선을 이루었습니다.

1.  **상태 공간 확장**: 에이전트가 **'각 행동에 대한 비용'** 을 직접 인지할 수 있도록 **`누적 클라이언트 처리 시간 벡터` 를 상태에 추가**했습니다. 이를 통해 에이전트는 '서버 대기 시간'과 '클라이언트 처리 시간' 사이의 트레이드오프를 명확히 이해하고, 정보에 기반한 효율적인 결정을 내릴 수 있게 되었습니다.

2.  **다운샘플링을 통한 차원 관리**: 확장된 '클라이언트 시간 벡터'의 차원이 너무 커지는 것을 방지하기 위해, 벡터를 10개씩 묶어 평균값을 사용하는 **다운샘플링** 기법을 적용했습니다. 이는 정보의 손실을 최소화하면서도 학습에 효율적인 상태 크기를 유지하는 역할을 합니다.

3.  **보상 함수 설계 및 학습 안정화**: 확장된 상태 정보를 효과적으로 학습시키기 위해, **턴어라운드 타임에 반비례하는 보상 함수** 를 도입하고 **어드밴티지 정규화, 경사도 클리핑** 등의 기법을 적용하여 학습 과정 전체의 안정성을 확보했습니다.(수정)

---

## 결론

본 실험을 통해 **강화 학습 기반의 PPO 에이전트** 가 제한적인 정보만을 사용하는 휴리스틱 모델보다 에지 컴퓨팅 환경에서의 작업 오프로딩 결정을 훨씬 더 효과적으로 수행할 수 있음을 확인했습니다. 특히 **상태 공간을 어떻게 설계하는지가 에이전트의 성능에 결정적인 영향을 미친다** 는 것을 입증했습니다.

---

## 향후 연구 방향

-   더욱 복잡하고 동적인 환경에서의 PPO 에이전트 성능 평가
-   다른 강화 학습 알고리즘과의 비교 연구
-   실제 에지 컴퓨팅 시스템에의 적용 가능성 및 문제점 분석
-   전송 시간, 네트워크 지연 등 추가적인 요소를 고려한 환경 모델링
